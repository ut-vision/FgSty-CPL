# ObMan-Ego
We created a ObMan-Ego dataset for sim-to-real adaptation of first-person hand segmentation. The detail is found here:
>The ObMan-Ego is a large-scale synthetic hand dataset with egocentric scenes in which the simulated hands are provided by ObMan. Training, validation, and testing sets contain 150, 000, 6, 500, and 6, 500 images, respectively. The ObMan is generated by Graspit, an automatic robotic grasping software, and ShapeNet object models. We rendered ObMan with the backgrounds of two large-scale egocentric videos, EPIC-KITCHENS100 and Something-Something. To collect egocentric scenes without hands, we eliminated frames in which hands appeared by using a hand detector and then used the remained frames for the rendering.

<img src="https://user-images.githubusercontent.com/28190044/124394118-e2ecaa00-dd38-11eb-8011-bee0b3c08960.jpeg">


## References
ObMan: https://github.com/hassony2/obman_render \
EPIC-KITCHENS100: https://epic-kitchens.github.io/2020-100 \
Something-Something: https://20bn.com/datasets/something-something
