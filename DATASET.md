# ObMan-Ego
We created a ObMan-Ego dataset for sim-to-real adaptation of first-person hand segmentation. The detail is found here:
>The ObMan-Ego is a large-scale synthetic hand dataset with egocentric scenes in which the simulated hands are provided by ObMan [20]. Training, validation, and testing sets contain 150, 000, 6, 500, and 6, 500 images, respectively. The ObMan is generated by Graspit [39], an automatic robotic grasping software and ShapeNet [52] object models. We rendered ObMan with the backgrounds of two large-scale egocentric videos, EPIC-KITCHENS100 [12] and Something-Something [19]. To collect egocentric scenes without hands, we eliminated frames in which hands appeared by using a hand detector [50] and then used the remained frames for the rendering.

<img src="https://user-images.githubusercontent.com/28190044/124394118-e2ecaa00-dd38-11eb-8011-bee0b3c08960.jpeg">


## References
ObMan: https://github.com/hassony2/obman_render \
EPIC-KITCHENS100: https://epic-kitchens.github.io/2020-100 \
Something-Something: https://20bn.com/datasets/something-something
